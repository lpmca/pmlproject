Pratical Machine Learning Course Project
========================================

```{r}
library(caret)
library(randomForest)
```

```{r}
train <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

```{r}
dim(train)
```
We have 160 variables, which we can find that many are largely filled with NA values.
```{r}
dim(testing)
```

```{r}
set.seed(52)
nzv <- nearZeroVar(train)
train<-train[,-nzv]
# Partition the training set into a training and cross validation set
inTrain <- createDataPartition(train$classe,p=0.7,list=FALSE)

xTrain <- train[inTrain,]
xCross <- train[-inTrain,]
```

```{r}
#We remove the first seven columns becuase these are all filled with information that is irrelevant to our analysis (username, time)
xTrain <- xTrain[,-c(1:7)]
xCross <- xCross[,-c(1:7)]
testing <- testing[,-c(1:7)]

#We remove the columns filled with more than ninety percent NA values
xTrain <- xTrain[,colSums(is.na(xTrain))<nrow(xTrain)*.9]
xTrain$classe <- as.factor(xTrain$classe)
xCross <- xCross[,colSums(is.na(xCross))<nrow(xCross)*.9]
xCross$classe <- as.factor(xCross$classe)
testing <- testing[,colSums(is.na(testing))<nrow(testing)*.9]
```

We perform resampling to better fit our model (prevent over-fitting) which we can do with trainControl
```{r}
control <- trainControl(method="cv",number=3,verboseIter = F)
```
We are doing random forest for classifying. We find an error rate of 1/6%.
```{r}
forest <- train(classe~.,data=xTrain,method="rf",trControl=control,tuneLength=5)
forest$finalModel
```
Here we can do cross validation where we have a similar error rate of 1.5%.
```{r}
pred_for <- predict(forest$finalModel,newdata=xCross)
confusionMatrix(pred_for,xCross$classe)
```
We have found a model that will fairly accurately predict class with about sample error of about 1.5%.